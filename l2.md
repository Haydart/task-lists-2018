# LSDP - Task list 2

## Vagrant, Ansible, bash, Zeppelin (Spark)

### Tech:

* [Ansible](https://www.ansible.com/)
* [Zeppelin](https://zeppelin.apache.org/)

### Tasks

* Generating data
* Word counting

0. Look at the Vagrantfile, and the Ansible playbook (if you have problems with Ansible, you can uncomment shell in Vagrantfile)

1. Bash:

	1. Create a script that can generate ```N``` random sentences (random a-z characters), parameters:
		- MIN, MAX sentence length (in words)
		- MIN, MAX word length
		- N - number of senteces
	
	One of the possible solutions: create a function that can generate a random word of given length by filtering the random stream (/dev/urandom), use that function to generate a random sentence by taking appropriate random numbers from given ranges (word and sentence length). You can use tools like shuf, fold, head, cat

	1. Generate two scripts that can count words from given sentences stream (the previous task, use pipe operator |). One can use ```wc```, the second one, any other solution


2. Zeppelin:

	0. Tak a look at Zeppelin UI on [http://localhost:8080](http://localhost:8080), have a look at tutorial notes

	1. Create a function that can generate random sentences (same parameters as in bash part, use prepared LSDP/l2 note)
	
	Idea: Parallelize array of N numbers (N = number of sentences), map each of the rows in new DataFrame to a random sentence. Think about the issues and limitations of that solution (randomness, max number of sentences)

	2. Count words in sentences using flatMap, split, count

	3. Count unique words in sentences, provide at least 2 implementations (distinct, count, map, countByKey, reduceByKey, treeAggregate)

	4. Count exsistence of each word in given sentences, provide at least 2 implementations (groupBy, treeAggregate, reduceByKey, [look here](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html))
