# LSDP - Task list 3

## Spark deployment

### Tech:

* [Spark](https://spark.apache.org/)

### Tasks

* [UCI Data Repository](https://archive.ics.uci.edu/ml/datasets.html)
* For all computations use Spark API, do not gather data in order to use standard Scala

0. Play with example spark project:
    
    * build jar (sbt package or sbt assembly if you need to generate jar that contains your dependencies)
    * deploy jar as spark job ([spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html), local master)
    * ```bash
        vagrant up
        vagrant ssh
        cd /project/example
        ./sbt package
        spark-submit --class ml.lsdp.example.Demo --master 'local[*]' target/scala-2.12/spark-example-assembly-0.0.1.jar
    * explain arguments: local[*], class, what other can you use?
    * find out how to turn off spark info logging
    * Do you need to create project, build it and run using spark-submit each time? (spark-shell, play with it)

1. Select dataset from UCI repository that has:

    * numeric attributes
    * categorical attributes
    * class attribute (>2 classes)
    * at least 1 000 000 instances

2. Create new spark project (you can use [rapid project start](https://developer.lightbend.com/start/), or copy example)
3. Create a case class that represents your data instance (single row). Name attributes appropriately
4. Load your data to spark as DataSet
	* spark.read
	* provide schema or use inference
	* convert dataframe to dataset using .as[<TYPE>]

5. Summarize your data using two methods:

    * built-in
    * implement your own summarization logic. A minimal set of attributes:

        * numerical:

            * min
            * max
            * mean
            * 95% percentile

        * categorical:

            * count of unique values
            * counts for each value
            * most common value
            * most uncommon value

6. Train a [classifier](https://spark.apache.org/docs/2.1.0/ml-guide.html) on your data:
    
    * split data into train and test
    * select a classifier
    * train it
    * evaluate classifier in two ways:

        * use built in evaluation metrics
        * implement your own evaluation metrics:

            * accuracy
            * F1
            * precision
            * recall

7. Prepare short markdown file with results:
	* data statistics
	* classifier metrics
	* execution times:
		* think about how to measure times of execution properly (JIT, SD, etc.)