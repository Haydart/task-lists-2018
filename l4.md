# LSDP - Task list 4

## Spark: text, cluster
### Tech:

* [Spark](https://spark.apache.org/)
* Scala (only)

### Tasks

* For all computations use Spark API, do not gather data in order to use standard Scala

0. Play with example spark project:
    
    * build jar (sbt package or sbt assembly if you need to generate jar that contains your dependencies)
    * deploy jar as spark job ([spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html), local master):
        * Build: 

            ```bash
                vagrant up
                vagrant ssh
                cp -r /project/example /tmp #building in shared folder is painfully slow
                cd /tmp/example
                ./sbt assembly
            ```
            
        * Deploy on vagrant: 

            ```bash
                vagrant ssh
                cd /tmp/example
                spark-submit --class ml.lsdp.example.Demo --master 'local[*]' target/scala-2.11/spark-example-assembly-0.0.1.jar
            ```
        * Deploy on cluster: 

            ```bash
                vagrant ssh
                cd /tmp/example
                scp -r /project/spark_pbs <user>@ui.wcss.pl:~/
                scp target/scala-2.11/spark-example-assembly-0.0.1.jar <user>@ui.wcss.pl:~/spark_pbs
                ssh <user>@ui.wcss.pl
                cd ~/spark_pbs
                qsub ./example_start.sh
                qstat -u <user>
            ```

1. Find text dataset for classification, sample sources:

    * [Kaggle](https://www.kaggle.com/datasets):
        * [IMDB Movie Review](https://www.kaggle.com/utathya/imdb-review-dataset)

2. Create new spark project (you can use [rapid project start](https://developer.lightbend.com/start/), or copy example)
3. Load data to Spark DataSet
4. Create processing pipeline:
    
    * Tokenize
    * Remove stop words
    * Vectorize using TF-IDF and Bag of Words

6. Train a [classifier](https://spark.apache.org/docs/2.1.0/ml-guide.html) on your data:
    
    * split data into train and test
    * select a classifier
    * train it
    * evaluate classifier using built in methods

7. Run on WCSS:
    

    * look (use) sample PBS script (project/spark_pbs/example_start.sh):

        * explain parameters

7. Prepare short markdown file with results:
	* data statistics
	* classifier metrics
	* execution times